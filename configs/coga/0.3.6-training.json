{
    "datasets": [
        {
            "data_class": "DineshChitChatDataset",
            "data_name": "chitchat",
            "data_path": "../multidoc2dial-data-forge/chitchat/v2/chitchat_v2/",
            "data_sampling_proportion": 6,
            "max_input_tokens": 1024,
            "max_output_tokens": 256,
            "dataset_type": "response_evidence",
            "combine_no_evidence": true,
            "files": {
                "train": "chitchat_subdocs_train.json",
                "val": "chitchat_subdocs_val.json",
                "test": "chitchat_document_test.json"
            }
        },
        {
            "data_class": "YatinAnswerabilityDataset",
            "data_name": "multidoc2dial",
            "data_path": "../multidoc2dial-data-forge/augmented_multidoc2dial/v3/contextual_unanswerable_classifier_with_da/",
            "data_sampling_proportion": 14,
            "max_input_tokens": 1024,
            "max_output_tokens": 256,
            "dataset_type": "response_evidence",
            "combine_no_evidence": true,
            "files": {
                "train": "md2d_subdocs_train_pos_neg.json",
                "val": "md2d_subdocs_val_pos_neg.json",
                "test": "md2d_document_test_pos_neg.json"
            }
        },
        {
            "data_class": "YatinAnswerabilityDataset",
            "data_name": "human 2 human",
            "data_path": "../multidoc2dial-data-forge/h2h/v3/coga_internal_v3_h2h_processed",
            "data_sampling_proportion": 5,
            "max_input_tokens": 1024,
            "max_output_tokens": 256,
            "dataset_type": "response_evidence",
            "combine_no_evidence": true,
            "filter_allowed": false,
            "files": {
                "train": "coga_internal_v3_train.json",
                "val": "coga_internal_v3_test.json",
                "test": "coga_internal_v3_test.json"
            }
        }
    ],
    "model_name": "google/flan-t5-xl",
    "model_class": "AutoModelForSeq2SeqLM",
    "training_inference_type": "full_finetuning",
    "logdir": "../aim-repo",
    "experiment_name": "coga-3b-0.3.6",
    "save_path": "../checkpoints/coga-3b-0.3.6",
    "num_training_steps": 20000,
    "eval_interval": 2000,
    "save_interval": 4000,
    "batch_size_per_gpu": 2,
    "dtype": "bfloat16",
    "optimizer": {
        "optimizer_class": "ApexFusedAdam",
        "lr": 1e-5,
        "weight_decay": 0.1,
        "betas": [0.9, 0.95],
        "eps": 1e-10
    },
    "lr_schedule": "cosine"
}
