{
    "datasets": [
        {
            "data_class": "DebugDataset",
            "data_sampling_proportion": 1,
            "max_input_tokens": 1024,
            "max_output_tokens": 128
        }
    ],
    "model_name": "google/flan-t5-xxl",
    "model_class": "AutoModelForSeq2SeqLM",
    "training_inference_type": "full_finetuning",
    "experiment_name": "gma-gpt2-xl-dolly",
    "save_path": "/cos/mayank/checkpoints/tmp",
    "num_training_steps": 1000,
    "eval_interval": 1600,
    "save_interval": 1600,
    "batch_size_per_gpu": 2,
    "dtype": "bfloat16",
    "eval_during_training": true,
    "gradient_accumulation_steps": 8,
    "learning_rate": 1e-5,
    "lr_schedule": "linear"
}
